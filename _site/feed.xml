<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The Math Bootcamp</title>
    <description>The homepage for Mila's Math Bootcamp</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>The Derivative</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;“Whatcha doing George?” “Oh nothing Lenny, just working out some gradients.” “On paper? I’m not sure if you’ll be able to call &lt;code class=&quot;highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt; there.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;Machine learning, especially deep learning, is built almost entirely on differentiation. In this post, we will briefly describe differentiation, derivatives, and gradients. From there, we will continue on to discuss their connection to the underlying core idea behind many popular deep learning frameworks: automatic differentiation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1pD4Lnr1Gs8S3KiUT86D1zZzpWlHOBWRQ&quot; target=&quot;_parent&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;derivatives&quot;&gt;Derivatives&lt;/h2&gt;

&lt;p&gt;While derivatives can be explained and generalized to quite &lt;a href=&quot;#TODO&quot;&gt;a&lt;/a&gt; &lt;a href=&quot;#TODO&quot;&gt;few&lt;/a&gt; &lt;a href=&quot;#TODO&quot;&gt;domains&lt;/a&gt;, we will start with the most elementary formulation: in the case of a function of a single, real-valued variable.&lt;/p&gt;

&lt;p&gt;Concretely, a derivative in this setting measures sensitivity of change of the output with respect to its input: graphically in one dimension, this is the slope of the tangent line at &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;; in physics, we often hear that the rate of change in position (output) with respect to time (input) is &lt;em&gt;velocity&lt;/em&gt;, and similarly for velocity and acceleration.&lt;/p&gt;

&lt;p&gt;![TODO - Links need to be fixed][../images/the-derivative/sincos.gif]&lt;/p&gt;

&lt;p&gt;Derivatives can be generalized to both higher orders and higher dimensions. Higher orders of derivatives are just differentiating with respect to the input once again; for example, the second order derivative of position with respect to time brings us back to acceleration.&lt;/p&gt;

&lt;p&gt;To understand higher dimensional derivatives, we can introduce &lt;em&gt;partial derivatives&lt;/em&gt;. When given vector-values input &lt;script type=&quot;math/tex&quot;&gt;\vec{x}&lt;/script&gt;, a partial derivative is just a derivative of the output with respect to a particular &lt;em&gt;component&lt;/em&gt; of &lt;script type=&quot;math/tex&quot;&gt;\vec{x}&lt;/script&gt;. When dealing with scalar-valued functions (i.e the output is a scalar, or notationally, &lt;script type=&quot;math/tex&quot;&gt;f: \mathbb{R}^n \rightarrow \mathbb{R}&lt;/script&gt;), the vector of first order partial derivatives is called the &lt;em&gt;gradient&lt;/em&gt;. If the output itself is vector-valued, the matrix of first-order partial derivatives is called the &lt;em&gt;Jacobian&lt;/em&gt;; in fact, the Jacobian is just a generalization of the gradient.&lt;/p&gt;

&lt;p&gt;Just as the derivative of a real-valued function is the slope of the tangent line, the gradient’s direction is the &lt;em&gt;direction of greatest increase&lt;/em&gt;, where the rate of that increase is given by the magnitude. Often, we will use the notation &lt;script type=&quot;math/tex&quot;&gt;\nabla f(\vec{x})&lt;/script&gt; to denote the gradient notationally.&lt;/p&gt;

&lt;p&gt;In machine learning, we are generally concerned with scalar-valued functions, so we focus our attention mostly on the gradient. While higher-order derivatives of scalar-valued functions are used in machine learning (the matrix of second-order derivatives of a vector-valued function is called the &lt;em&gt;Hessian&lt;/em&gt;, and when the derivatives are taken with respect to parameters instead of the inputs,the matrix is called the &lt;em&gt;Fisher Information Matrix&lt;/em&gt;), they deserve to be covered in their own post, and are not central to discussion here.&lt;/p&gt;

&lt;h2 id=&quot;machine-learning-and-gradients&quot;&gt;Machine Learning and Gradients&lt;/h2&gt;

&lt;p&gt;In machine learning, we care about &lt;em&gt;objective functions&lt;/em&gt; and how to optimize them, whether we are working with supervised, deep, reinforcement, … learning. To optimize a function, we look for &lt;em&gt;optima&lt;/em&gt;, or where &lt;script type=&quot;math/tex&quot;&gt;\nabla f(\vec{x}) = 0&lt;/script&gt; and the second-derivative test passes with the condition we’re looking for.&lt;/p&gt;

&lt;p&gt;While generally, we can’t make the assumption that our objective function is convex (which means that we can’t guarantee that any local optima are globally optimal), we can still use gradients to find local minima (or maxima, depending on the problem we’re trying to solve).&lt;/p&gt;

&lt;p&gt;This gives rise to algorithms like &lt;em&gt;gradient descent&lt;/em&gt; (or more popularly, the more efficient &lt;em&gt;stochastic&lt;/em&gt; version); since we know that our gradient gives us the direction of fastest increase, we (to minimize an objective) can follow it in the ecact opposite direction.&lt;/p&gt;

&lt;p&gt;While just following gradients can get us into trouble as well (saddles also have &lt;script type=&quot;math/tex&quot;&gt;\nabla f(\vec{x}) = 0&lt;/script&gt; just like optima, and unfortunately, it seems that &lt;a href=&quot;#TODO&quot;&gt;saddles are ubiquitous in nonconvex optimization&lt;/a&gt;; SGD is generally considered to be effective at escaping these saddles, but the dynamics of gradient descent algorithms is still an &lt;a href=&quot;#TODO&quot;&gt;active area of research&lt;/a&gt;), we defer those discussions to a later post.&lt;/p&gt;

&lt;h2 id=&quot;calculating-gradients-numerically&quot;&gt;Calculating Gradients, Numerically&lt;/h2&gt;

&lt;p&gt;To understand the numerical calculation of gradients, we must refer back to the definition of derivatives, in terms of limits:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f'(a) = \lim_{h\rightarrow 0} \frac{f(a + h) - f(a)}{h}&lt;/script&gt;

&lt;p&gt;However, since limits are a theoretical tool, they offer us no help in the calculation of gradients in practice.&lt;/p&gt;

&lt;p&gt;Numerically, we can replace the infinitetesimal &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; with an arbitrarily small one, leading us to approximate the derivative as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f'(a) \approx \lim_{h\rightarrow 0} \frac{f(a + h) - f(a)}{h}&lt;/script&gt;

&lt;p&gt;known as a &lt;em&gt;first-order divided difference&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Numerical differentiation (known as &lt;em&gt;differential quadrature&lt;/em&gt;) are still the leading methods to solve partial differential equations (and are used to calculate derivatives in your favorite graphing calculator as well!), they incur errors on an order of magnitude of &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;; first-order divided differences incurs errors on the order of &lt;script type=&quot;math/tex&quot;&gt;O(h)&lt;/script&gt;, and while improved methods such as symmetric divided difference can lower this, differential quadrature methods can be prone to floating point approximation errors and implementation efficiency issues.&lt;/p&gt;

&lt;h2 id=&quot;calculating-gradients-automagically&quot;&gt;Calculating Gradients, Automagically&lt;/h2&gt;

&lt;h3 id=&quot;from-symbolic-to-automatic&quot;&gt;From Symbolic to Automatic&lt;/h3&gt;

&lt;h3 id=&quot;the-chain-rule&quot;&gt;The Chain Rule&lt;/h3&gt;

&lt;h3 id=&quot;sum-of-the-parts-are-greater-than-the-whole&quot;&gt;Sum of the Parts are Greater than the Whole&lt;/h3&gt;

&lt;p&gt;A section on how simple arithmetic operations and simple functions can be composed to efficiently calculate derivatives.&lt;/p&gt;

&lt;h3 id=&quot;forward-mode-and-backward-mode&quot;&gt;Forward Mode and Backward Mode&lt;/h3&gt;

</description>
        <pubDate>Tue, 23 Jul 2019 08:22:18 -0400</pubDate>
        <link>http://localhost:4000/machinelearning/basics/2019/07/23/the-derivative/</link>
        <guid isPermaLink="true">http://localhost:4000/machinelearning/basics/2019/07/23/the-derivative/</guid>
      </item>
    
  </channel>
</rss>
